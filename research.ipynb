{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "The main goal of the project to create a Data Lake in S3 using Airflow trigger Spark.  \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"DataEngineeringCapstoneProject\").getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder.\\\n",
    "# config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "# .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of project is to build a Data Mode to analysis CVID-19 world vaccination progress. For example We can analysis where need to speed up vaccination progress.\n",
    "\n",
    "#### The following technologies were used: \n",
    "- Spark\n",
    "- Airflow\n",
    "- AWS EMR S3 \n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "##### 1.The Data sources:\n",
    "\n",
    "The Daily and Total Vaccination for COVID-19 in the world is provided by Kaggle\n",
    "- [country_vaccinations.csv](https://www.kaggle.com/gpreda/covid-world-vaccination-progress)\n",
    "\n",
    "WHO Coronavirus Disease (COVID-19) is provided by WHO\n",
    "- [WHO-COVID-19-global-data.csv](https://covid19.who.int/)\n",
    "\n",
    "ISO country code provide by Kaggle\n",
    "- [country_code.csv](https://www.kaggle.com/koki25ando/country-code)\n",
    "\n",
    "The Data contains 12 columns provide by Kaggle\n",
    " - [ Countries_usefulFeatures.csv](https://www.kaggle.com/ishivinal/covid19-useful-features-by-country)\n",
    " \n",
    "The Data extracted from Wikipedia's list of countries by category is provided by Kaggle \n",
    " - [WORLD DATA by country (2020)](https://www.kaggle.com/daniboy370/world-data-by-country-2020?select=Life+expectancy.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Gather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.session.timeZone', 'UTC') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_vaccinations_spark = spark.read.csv(\"data/country_vaccinations.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The data contains the following information:\n",
    "\n",
    " - Country- this is the country for which the vaccination information is provided;\n",
    " - Country ISO Code - ISO code for the country;\n",
    " - Date - date for the data entry; for some of the dates we have only the daily vaccinations, for others, only the (cumulative) total;\n",
    " - Total number of vaccinations - this is the absolute number of total immunizations in the country;\n",
    " - Total number of people vaccinated - a person, depending on the immunization scheme, will receive one or more (typically 2) vaccines; at a certain moment, the number of vaccination might be larger than the number of people;\n",
    " - Total number of people fully vaccinated - this is the number of people that received the entire set of immunization according to the immunization scheme (typically 2); at a certain moment in time, there might be a certain number of people that received one vaccine and another number (smaller) of people that received all vaccines in the scheme;\n",
    " - Daily vaccinations (raw) - for a certain data entry, the number of vaccination for that date/country;\n",
    " - Daily vaccinations - for a certain data entry, the number of vaccination for that date/country;\n",
    " - Total vaccinations per hundred - ratio (in percent) between vaccination number and total population up to the date in the country;\n",
    " - Total number of people vaccinated per hundred - ratio (in percent) between population immunized and total population up to the date in the country;\n",
    " - Total number of people fully vaccinated per hundred - ratio (in percent) between population fully immunized and total population up to the date in the country;\n",
    " - Number of vaccinations per day - number of daily vaccination for that day and country;\n",
    " - Daily vaccinations per million - ratio (in ppm) between vaccination number and total population for the current date in the country;\n",
    " - Vaccines used in the country - total number of vaccines used in the country (up to date);\n",
    " - Source name - source of the information (national authority, international organization, local organization etc.);\n",
    " - Source website - website of the source of information;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_vaccinations_spark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add columns *year*  *month*  *week*  for aggregating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COVID19_vaccinations_spark = COVID19_vaccinations_spark.withColumn('year', func.year('date'))\\\n",
    "#                               .withColumn('month', func.month('date'))\\\n",
    "#                               .withColumn('week_of_year', func.weekofyear('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_vaccinations_spark.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_global_spark = spark.read.csv(\"data/WHO-COVID-19-global-data.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_global_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_global_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_spark = spark.read.csv(\"data/country_code.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_usefulFeatures_spark = spark.read.csv(\"data/Countries_usefulFeatures.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Country_Region: Name of the country\n",
    "- Population_Size: the population size 2018 stats\n",
    "- Tourism: International tourism, number of arrivals 2018\n",
    "- Date_FirstFatality: Date of the first Fatality of the COVID-19\n",
    "- Date_FirstConfirmedCase: Date of the first confirmed case of the COVID-19\n",
    "- Latitude\n",
    "- Longitude\n",
    "- Mean_Age: mean age of the population 2018 stats\n",
    "- Lockdown_Date: date of the lockdown\n",
    "- Lockdown_Type: type of the lockdown\n",
    "- Country_Code: 3 digit country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COVID19_usefulFeatures_spark.printSchema()\n",
    "COVID19_usefulFeatures_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_GDP_spark = spark.read.csv(\"data/WORLD DATA by country (2020)/GDP per capita.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_GDP_spark.printSchema()\n",
    "country_GDP_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_life_expectancy_spark = spark.read.csv(\"data/WORLD DATA by country (2020)/Life expectancy.csv\",sep=\",\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_life_expectancy_spark.printSchema()\n",
    "country_life_expectancy_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_median_age_spark = spark.read.csv(\"data/WORLD DATA by country (2020)/Median age.csv\",sep=\",\", inferSchema=True, header=True)\n",
    "country_median_age_spark.printSchema()\n",
    "country_median_age_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_population_growth_spark = spark.read.csv(\"data/WORLD DATA by country (2020)/Population growth.csv\",sep=\",\", inferSchema=True, header=True)\n",
    "country_population_growth_spark.printSchema()\n",
    "country_population_growth_spark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_urbanization_rate_spark = spark.read.csv(\"data/WORLD DATA by country (2020)/Urbanization rate.csv\",sep=\",\", inferSchema=True, header=True)\n",
    "country_urbanization_rate_spark.printSchema()\n",
    "country_urbanization_rate_spark.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 Explore and Cleaning  *COVID19_vaccinations_spark DataFrame*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Null value columns\n",
    "count = COVID19_vaccinations_spark.count()\n",
    "{col: f'{ COVID19_vaccinations_spark.filter(COVID19_vaccinations_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in COVID19_vaccinations_spark.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'iso_code' fill NULL as iso country code. Other column null value are filled with 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_null = COVID19_vaccinations_spark.select(\"country\",\"iso_code\")\\\n",
    "                          .filter(COVID19_vaccinations_spark[\"iso_code\"].isNull()).groupBy(\"country\").count()\n",
    "dict_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccinations_cleaned = COVID19_vaccinations_spark.fillna({\"iso_code\":\"GBR\", \"total_vaccinations\":0.0, \"people_vaccinated\":0.0, \"people_fully_vaccinated\":0.0,\\\n",
    "                       \"daily_vaccinations_raw\":0.0, \"daily_vaccinations\":0.0, \"total_vaccinations_per_hundred\":0.0,\\\n",
    "                                    \"people_vaccinated_per_hundred\":0.0, \"people_fully_vaccinated_per_hundred\":0.0,\\\n",
    "                                    \"daily_vaccinations_per_million\":0.0\n",
    "                                   })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Explore and Cleaning *COVID19_global_spark* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Null value columns\n",
    "count = COVID19_global_spark.count()\n",
    "{col: f'{ COVID19_global_spark.filter(COVID19_global_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in COVID19_global_spark.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_data_clean = COVID19_global_spark\n",
    "global_data_clean.select(\"country\",\"Country_code\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The country code is 2 digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Explore and Cleaning *COVID19_usefulFeatures_spark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the Null value columns\n",
    "count = COVID19_usefulFeatures_spark.count()\n",
    "{col: f'{ COVID19_usefulFeatures_spark.filter(COVID19_usefulFeatures_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in COVID19_usefulFeatures_spark.columns}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NULL values are meaningful,Just Keeping \n",
    "- Date_FirstFatality: Date of the first Fatality of the COVID-19\n",
    "- Lockdown_Date: date of the lockdown\n",
    "- Lockdown_Type: type of the lockdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usefulFeatures_cleaned = COVID19_usefulFeatures_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4 Explore and Cleaning *WORLD DATA by country (2020)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = country_GDP_spark.count()\n",
    "{col: f'{ country_GDP_spark.filter(country_GDP_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in country_GDP_spark.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_GDP_cleaned = country_GDP_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = country_life_expectancy_spark.count()\n",
    "{col: f'{ country_life_expectancy_spark.filter(country_life_expectancy_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in country_life_expectancy_spark.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_life_expectancy_cleaned = country_life_expectancy_spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = country_median_age_spark.count()\n",
    "{col: f'{ country_median_age_spark.filter(country_median_age_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in country_median_age_spark.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_null = country_median_age_spark.select(\"Country\",\"ISO-code\")\\\n",
    "                          .filter(country_median_age_spark[\"ISO-code\"].isNull()).groupBy(\"country\").count()\n",
    "dict_null.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_median_age_cleaned = country_median_age_spark.fillna({\"ISO-code\":\"VGB\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = country_population_growth_spark.count()\n",
    "{col: f'{ country_population_growth_spark.filter(country_population_growth_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in country_population_growth_spark.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_population_growth_cleaned = country_population_growth_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = country_urbanization_rate_spark.count()\n",
    "{col: f'{ country_urbanization_rate_spark.filter(country_urbanization_rate_spark[col].isNull()).count()/count *100 :.2f}%'\\\n",
    "for col in country_urbanization_rate_spark.columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_urbanization_rate_cleaned = country_urbanization_rate_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The dimension table is the entry point for data, There are four tables *country_region_dim*  *time_dim*  *vaccines_dim*   *source_dim*  and each measurement event in the world has a one-to-one relationship with the corresponding fact table row. There is one fact table *vaccinations_fact*.\n",
    "\n",
    "![DataModel.jpg](image/DataModel.jpg)\n",
    "\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DataModel.jpg](image/ETL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "- LICENSE\n",
    "- README.md\n",
    "- data   # raw data files\n",
    "- image # project relevant images\n",
    "- docker-airflow #  DAG files \n",
    "- research.ipynb # The project jupyter notebook file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Install Airflow Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Download docker and installing\n",
    "https://www.docker.com/products/docker-desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Installation docker-airflow \n",
    "```\n",
    "docker pull puckel/docker-airflow\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Build docker-airflow\n",
    "```\n",
    "cd <project path>/docker-airflow/\n",
    "\n",
    "docker build --rm --build-arg AIRFLOW_DEPS=\"datadog,dask\" -t puckel/docker-airflow .\n",
    "\n",
    "docker build --rm --build-arg PYTHON_DEPS=\"flask_oauthlib>=0.9\" -t puckel/docker-airflow .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 3. Run Airflow Docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "docker-compose -f docker-compose-LocalExecutor.yml up -d\n",
    "\n",
    "docker ps\n",
    "\n",
    "docker exec -it <container_id> /bin/bash\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8080/admin/\n",
    "\n",
    "<img src=\"image/localhost.jpg\" width=\"400\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
